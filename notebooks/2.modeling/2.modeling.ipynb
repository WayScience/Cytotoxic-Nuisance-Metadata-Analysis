{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2: Modeling \n",
    "\n",
    "In this notebook, we focus on developing and training a Multi-class logistic regression model, employing Randomized Cross-Validation (CV) for hyperparameter tuning to address our classification task. The dataset is split into an 80% training set and a 20% testing set. To evaluate the performance of our model during training, we used performance evaluation metrics such as precision, recall, and F1 scores. Additionally, we extend our evaluation by testing our model on a holdout dataset, which includes plate, treatment, and well information, providing a comprehensive assessment of its real-world performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pathlib\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# import local modules\n",
    "sys.path.append(\"../../\")\n",
    "from src.utils import (\n",
    "    train_multiclass,\n",
    "    shuffle_features,\n",
    "    load_json_file,\n",
    "    check_feature_order,\n",
    "    generate_confusion_matrix_tl,\n",
    "    evaluate_model_performance,\n",
    "    split_meta_and_features,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting random seeds variables\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "\n",
    "# setting paths and parameters\n",
    "results_dir = pathlib.Path(\"../../results\").resolve(strict=True)\n",
    "feature_dir = (results_dir / \"0.feature_selection/\").resolve(strict=True)\n",
    "data_splits_dir = (results_dir / \"1.data_splits\").resolve(strict=True)\n",
    "\n",
    "# test and train data paths\n",
    "X_train_path = (data_splits_dir / \"X_train.csv.gz\").resolve(strict=True)\n",
    "X_test_path = (data_splits_dir / \"X_test.csv.gz\").resolve(strict=True)\n",
    "y_train_path = (data_splits_dir / \"y_train.csv.gz\").resolve(strict=True)\n",
    "y_test_path = (data_splits_dir / \"y_test.csv.gz\").resolve(strict=True)\n",
    "\n",
    "# shared feature space path\n",
    "feature_space_path = (feature_dir / \"cell_injury_shared_feature_space.json\").resolve(\n",
    "    strict=True\n",
    ")\n",
    "\n",
    "# holdout paths\n",
    "plate_holdout_path = (data_splits_dir / \"plate_holdout.csv.gz\").resolve(strict=True)\n",
    "treatment_holdout_path = (data_splits_dir / \"treatment_holdout.csv.gz\").resolve(\n",
    "    strict=True\n",
    ")\n",
    "wells_holdout_path = (data_splits_dir / \"wells_holdout.csv.gz\").resolve(strict=True)\n",
    "\n",
    "# setting output paths\n",
    "modeling_dir = (results_dir / \"2.modeling\").resolve()\n",
    "modeling_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# setting cv score output paths for both shuffled and non-shuffled models\n",
    "cv_outpath = (modeling_dir / \"multi_class_cv_results.csv\").resolve()\n",
    "shuffled_cv_outpath = (modeling_dir / \"shuffled_multi_class_cv_results.csv\").resolve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the parameters used:\n",
    "\n",
    "- **penalty**: Specifies the type of penalty (regularization) applied during logistic regression. It can be 'l1' for L1 regularization, 'l2' for L2 regularization, or 'elasticnet' for a combination of both.\n",
    "- **C**: Inverse of regularization strength; smaller values specify stronger regularization. Controls the trade-off between fitting the training data and preventing overfitting.\n",
    "- **max_iter**: Maximum number of iterations for the optimization algorithm to converge.\n",
    "- **tol**: Tolerance for the stopping criterion during optimization. It represents the minimum change in coefficients between iterations that indicates convergence.\n",
    "- **l1_ratio**: The mixing parameter for elastic net regularization. It determines the balance between L1 and L2 penalties in the regularization term. A value of 1 corresponds to pure L1 (Lasso) penalty, while a value of 0 corresponds to pure L2 (Ridge) penalty\n",
    "- **solver**: Optimization algorithms to be explored during hyperparameter tuning for logistic regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "param_grid = {\n",
    "    \"penalty\": [\"l1\", \"l2\", \"elasticnet\"],\n",
    "    \"C\": [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    \"max_iter\": np.arange(100, 1100, 100),\n",
    "    \"tol\": np.arange(1e-6, 1e-3, 1e-6),\n",
    "    \"l1_ratio\": [0.1, 0.3, 0.5, 0.7, 0.9],\n",
    "    \"solver\": [\"newton-cg\", \"lbfgs\", \"liblinear\", \"sag\", \"saga\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading training data splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X training size (10802, 221)\n",
      "X testing size (2701, 221)\n",
      "y training size (10802, 1)\n",
      "y testing size (2701, 1)\n"
     ]
    }
   ],
   "source": [
    "# loading injury codes\n",
    "injury_codes = load_json_file(feature_dir / \"injury_codes.json\")\n",
    "\n",
    "# load share feature space data\n",
    "feature_space = load_json_file(feature_space_path)\n",
    "shared_features = feature_space[\"features\"]\n",
    "\n",
    "# loading training data splits\n",
    "X_train = pd.read_csv(X_train_path)\n",
    "X_test = pd.read_csv(X_test_path)\n",
    "y_train = pd.read_csv(y_train_path)\n",
    "y_test = pd.read_csv(y_test_path)\n",
    "\n",
    "\n",
    "# splitting meta and feature column names\n",
    "_, feat_cols = split_meta_and_features(X_train)\n",
    "\n",
    "# checking if the feature space are identical (also looks for feature space order)\n",
    "assert check_feature_order(\n",
    "    ref_feat_order=shared_features, input_feat_order=X_test.columns.tolist()\n",
    "), \"Feature space are not identical\"\n",
    "\n",
    "# display data split sizes\n",
    "print(\"X training size\", X_train.shape)\n",
    "print(\"X testing size\", X_test.shape)\n",
    "print(\"y training size\", y_train.shape)\n",
    "print(\"y testing size\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluating Multi-class Logistic Model with original dataset split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting model path\n",
    "model_path = modeling_dir / \"multi_class_model.joblib\"\n",
    "\n",
    "# if trained model exists, skip training\n",
    "if model_path.exists():\n",
    "    best_model = joblib.load(model_path)\n",
    "\n",
    "# train model and save\n",
    "else:\n",
    "    best_model = train_multiclass(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        param_grid=param_grid,\n",
    "        cv_results_outpath=cv_outpath,\n",
    "        seed=seed,\n",
    "    )\n",
    "    joblib.dump(best_model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating model on train dataset\n",
    "train_precision_recall_df, train_f1_score_df = evaluate_model_performance(\n",
    "    model=best_model, X=X_train, y=y_train, shuffled=False, dataset_type=\"Train\"\n",
    ")\n",
    "\n",
    "# evaluating model on test dataset\n",
    "test_precision_recall_df, test_f1_score_df = evaluate_model_performance(\n",
    "    model=best_model, X=X_test, y=y_test, shuffled=False, dataset_type=\"Test\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating confusion matrix for both train and test set on non-shuffled model\n",
    "cm_train_df = generate_confusion_matrix_tl(\n",
    "    model=best_model, X=X_train, y=y_train, shuffled=False, dataset_type=\"Train\"\n",
    ")\n",
    "cm_test_df = generate_confusion_matrix_tl(\n",
    "    model=best_model, X=X_test, y=y_test, shuffled=False, dataset_type=\"Test\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluating Multi-class Logistic Model with shuffled dataset split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle feature space\n",
    "shuffled_X_train = shuffle_features(X_train, features=shared_features, seed=seed)\n",
    "\n",
    "# checking if the shuffled and original feature space are the same\n",
    "assert not X_train.equals(shuffled_X_train), \"DataFrames are the same!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting model path\n",
    "shuffled_model_path = modeling_dir / \"shuffled_multi_class_model.joblib\"\n",
    "\n",
    "# if trained model exists, skip training\n",
    "if shuffled_model_path.exists():\n",
    "    shuffled_best_model = joblib.load(shuffled_model_path)\n",
    "\n",
    "# train model and save\n",
    "else:\n",
    "    shuffled_best_model = train_multiclass(\n",
    "        shuffled_X_train,\n",
    "        y_train,\n",
    "        param_grid=param_grid,\n",
    "        cv_results_outpath=shuffled_cv_outpath,\n",
    "        seed=seed,\n",
    "    )\n",
    "    joblib.dump(shuffled_best_model, shuffled_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating shuffled model on train dataset\n",
    "shuffle_train_precision_recall_df, shuffle_train_f1_score_df = (\n",
    "    evaluate_model_performance(\n",
    "        model=shuffled_best_model,\n",
    "        X=shuffled_X_train,\n",
    "        y=y_train,\n",
    "        shuffled=True,\n",
    "        dataset_type=\"Train\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# valuating shuffled model on test dataset\n",
    "shuffle_test_precision_recall_df, shuffle_test_f1_score_df = evaluate_model_performance(\n",
    "    model=shuffled_best_model, X=X_test, y=y_test, shuffled=True, dataset_type=\"Test\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating confusion matrix for shuffled model\n",
    "shuffled_cm_train_df = generate_confusion_matrix_tl(\n",
    "    model=shuffled_best_model,\n",
    "    X=shuffled_X_train,\n",
    "    y=y_train,\n",
    "    shuffled=True,\n",
    "    dataset_type=\"Train\",\n",
    ")\n",
    "shuffled_cm_test_df = generate_confusion_matrix_tl(\n",
    "    model=shuffled_best_model, X=X_test, y=y_test, shuffled=True, dataset_type=\"Test\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Multi-class model with holdout data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading in all the hold out data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading all holdouts\n",
    "plate_holdout_df = pd.read_csv(plate_holdout_path)\n",
    "treatment_holdout_df = pd.read_csv(treatment_holdout_path)\n",
    "well_holdout_df = pd.read_csv(wells_holdout_path)\n",
    "\n",
    "# splitting the dataset into X = features , y = injury_types\n",
    "X_plate_holdout = plate_holdout_df[feat_cols]\n",
    "y_plate_holdout = plate_holdout_df[\"injury_code\"]\n",
    "\n",
    "X_treatment_holdout = treatment_holdout_df[feat_cols]\n",
    "y_treatment_holdout = treatment_holdout_df[\"injury_code\"]\n",
    "\n",
    "X_well_holdout = well_holdout_df[feat_cols]\n",
    "y_well_holdout = well_holdout_df[\"injury_code\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Multi-class model trained with original split with holdout data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating plate holdout data with both trained original and shuffled model\n",
    "plate_ho_precision_recall_df, plate_ho_f1_score_df = evaluate_model_performance(\n",
    "    model=best_model,\n",
    "    X=X_plate_holdout,\n",
    "    y=y_plate_holdout,\n",
    "    shuffled=False,\n",
    "    dataset_type=\"Plate Holdout\",\n",
    ")\n",
    "\n",
    "plate_ho_shuffle_precision_recall_df, plate_ho_shuffle_f1_score_df = (\n",
    "    evaluate_model_performance(\n",
    "        model=shuffled_best_model,\n",
    "        X=X_plate_holdout,\n",
    "        y=y_plate_holdout,\n",
    "        shuffled=True,\n",
    "        dataset_type=\"Plate Holdout\",\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# evaluating treatment holdout data with both trained original and shuffled model\n",
    "treatment_ho_precision_recall_df, treatment_ho_f1_score_df = evaluate_model_performance(\n",
    "    model=best_model,\n",
    "    X=X_treatment_holdout,\n",
    "    y=y_treatment_holdout,\n",
    "    shuffled=False,\n",
    "    dataset_type=\"Treatment Holdout\",\n",
    ")\n",
    "\n",
    "treatment_ho_shuffle_precision_recall_df, treatment_ho_shuffle_f1_score_df = (\n",
    "    evaluate_model_performance(\n",
    "        model=shuffled_best_model,\n",
    "        X=X_treatment_holdout,\n",
    "        y=y_treatment_holdout,\n",
    "        shuffled=True,\n",
    "        dataset_type=\"Treatment Holdout\",\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# evaluating well holdout data with both trained original and shuffled model\n",
    "well_ho_precision_recall_df, well_ho_f1_score_df = evaluate_model_performance(\n",
    "    model=best_model,\n",
    "    X=X_well_holdout,\n",
    "    y=y_well_holdout,\n",
    "    shuffled=False,\n",
    "    dataset_type=\"Well Holdout\",\n",
    ")\n",
    "\n",
    "well_ho_shuffle_precision_recall_df, well_ho_shuffle_f1_score_df = (\n",
    "    evaluate_model_performance(\n",
    "        model=shuffled_best_model,\n",
    "        X=X_well_holdout,\n",
    "        y=y_well_holdout,\n",
    "        shuffled=True,\n",
    "        dataset_type=\"Well Holdout\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating confusion matrix with plate holdout (shuffled and not shuffled)\n",
    "plate_ho_cm_df = generate_confusion_matrix_tl(\n",
    "    model=best_model,\n",
    "    X=X_plate_holdout,\n",
    "    y=y_plate_holdout,\n",
    "    shuffled=False,\n",
    "    dataset_type=\"Plate Holdout\",\n",
    ")\n",
    "shuffled_plate_ho_cm_df = generate_confusion_matrix_tl(\n",
    "    model=shuffled_best_model,\n",
    "    X=X_plate_holdout,\n",
    "    y=y_plate_holdout,\n",
    "    shuffled=True,\n",
    "    dataset_type=\"Plate Holdout\",\n",
    ")\n",
    "\n",
    "# creating confusion matrix with treatment holdout (shuffled and not shuffled)\n",
    "treatment_ho_cm_df = generate_confusion_matrix_tl(\n",
    "    model=best_model,\n",
    "    X=X_treatment_holdout,\n",
    "    y=y_treatment_holdout,\n",
    "    shuffled=False,\n",
    "    dataset_type=\"Treatment Holdout\",\n",
    ")\n",
    "shuffled_treatment_ho_cm_df = generate_confusion_matrix_tl(\n",
    "    model=shuffled_best_model,\n",
    "    X=X_treatment_holdout,\n",
    "    y=y_treatment_holdout,\n",
    "    shuffled=True,\n",
    "    dataset_type=\"Treatment Holdout\",\n",
    ")\n",
    "\n",
    "# creating confusion matrix with plate_hold (shuffled and not shuffled)\n",
    "well_ho_cm_df = generate_confusion_matrix_tl(\n",
    "    model=best_model,\n",
    "    X=X_well_holdout,\n",
    "    y=y_well_holdout,\n",
    "    shuffled=False,\n",
    "    dataset_type=\"Well Holdout\",\n",
    ")\n",
    "shuffled_well_ho_cm_df = generate_confusion_matrix_tl(\n",
    "    model=shuffled_best_model,\n",
    "    X=X_well_holdout,\n",
    "    y=y_well_holdout,\n",
    "    shuffled=True,\n",
    "    dataset_type=\"Well Holdout\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing all f1 and pr scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing all f1 scores\n",
    "all_f1_scores = pd.concat(\n",
    "    [\n",
    "        # original split\n",
    "        test_f1_score_df,\n",
    "        train_f1_score_df,\n",
    "        # shuffle split\n",
    "        shuffle_test_f1_score_df,\n",
    "        shuffle_train_f1_score_df,\n",
    "        # plate holdout\n",
    "        plate_ho_f1_score_df,\n",
    "        plate_ho_shuffle_f1_score_df,\n",
    "        # treatment holdout\n",
    "        treatment_ho_f1_score_df,\n",
    "        treatment_ho_shuffle_f1_score_df,\n",
    "        # well holdout\n",
    "        well_ho_f1_score_df,\n",
    "        well_ho_shuffle_f1_score_df,\n",
    "    ]\n",
    ")\n",
    "\n",
    "# saving all f1 scores\n",
    "all_f1_scores.to_csv(\n",
    "    modeling_dir / \"all_f1_scores.csv.gz\", index=False, compression=\"gzip\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing pr scores\n",
    "all_pr_scores = pd.concat(\n",
    "    [\n",
    "        # original split\n",
    "        test_precision_recall_df,\n",
    "        train_precision_recall_df,\n",
    "        # shuffled split\n",
    "        shuffle_test_precision_recall_df,\n",
    "        shuffle_train_precision_recall_df,\n",
    "        # plate holdout\n",
    "        plate_ho_precision_recall_df,\n",
    "        plate_ho_shuffle_precision_recall_df,\n",
    "        # treatment holdout\n",
    "        treatment_ho_precision_recall_df,\n",
    "        treatment_ho_shuffle_precision_recall_df,\n",
    "        # well holdout\n",
    "        well_ho_precision_recall_df,\n",
    "        well_ho_shuffle_precision_recall_df,\n",
    "    ]\n",
    ")\n",
    "\n",
    "# saving pr scores\n",
    "all_pr_scores.to_csv(\n",
    "    modeling_dir / \"precision_recall_scores.csv.gz\", index=False, compression=\"gzip\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cm_dfs = pd.concat(\n",
    "    [\n",
    "        # original split\n",
    "        cm_train_df,\n",
    "        cm_test_df,\n",
    "        # shuffled split\n",
    "        shuffled_cm_train_df,\n",
    "        shuffled_cm_test_df,\n",
    "        # plate holdout\n",
    "        plate_ho_cm_df,\n",
    "        shuffled_plate_ho_cm_df,\n",
    "        # treatment holdout\n",
    "        treatment_ho_cm_df,\n",
    "        shuffled_treatment_ho_cm_df,\n",
    "        # well holdout\n",
    "        well_ho_cm_df,\n",
    "        shuffled_well_ho_cm_df,\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# saving pr scores\n",
    "all_cm_dfs.to_csv(\n",
    "    modeling_dir / \"confusion_matrix.csv.gz\", index=False, compression=\"gzip\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we combine the cross-validation (CV) scores from both the non-shuffled and shuffled models into a single dataset. The resulting concatenated `cv_score` file will include all the CV scores and solvers used during the training of our multi-class regression model, forming a comprehensive table. This table will contain a `model_type` column, which specifies the model being hyperparameterized during the execution of `RandomSearchCV`. The generated file will be saved in the `./results/2.modeling` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_tol</th>\n",
       "      <th>param_solver</th>\n",
       "      <th>param_penalty</th>\n",
       "      <th>param_max_iter</th>\n",
       "      <th>param_l1_ratio</th>\n",
       "      <th>param_C</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>model_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56.461415</td>\n",
       "      <td>2.210524</td>\n",
       "      <td>0.015963</td>\n",
       "      <td>0.002591</td>\n",
       "      <td>0.000406</td>\n",
       "      <td>saga</td>\n",
       "      <td>elasticnet</td>\n",
       "      <td>900</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.010</td>\n",
       "      <td>{'tol': 0.000406, 'solver': 'saga', 'penalty':...</td>\n",
       "      <td>0.378528</td>\n",
       "      <td>0.418325</td>\n",
       "      <td>0.395833</td>\n",
       "      <td>0.387963</td>\n",
       "      <td>0.443056</td>\n",
       "      <td>0.404741</td>\n",
       "      <td>0.023240</td>\n",
       "      <td>3</td>\n",
       "      <td>not shuffled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.021851</td>\n",
       "      <td>0.002654</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>elasticnet</td>\n",
       "      <td>800</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.100</td>\n",
       "      <td>{'tol': 0.00010499999999999999, 'solver': 'new...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>not shuffled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>195.807875</td>\n",
       "      <td>3.518417</td>\n",
       "      <td>0.015172</td>\n",
       "      <td>0.003087</td>\n",
       "      <td>0.000468</td>\n",
       "      <td>saga</td>\n",
       "      <td>elasticnet</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.100</td>\n",
       "      <td>{'tol': 0.000468, 'solver': 'saga', 'penalty':...</td>\n",
       "      <td>0.673762</td>\n",
       "      <td>0.676539</td>\n",
       "      <td>0.663889</td>\n",
       "      <td>0.652778</td>\n",
       "      <td>0.707407</td>\n",
       "      <td>0.674875</td>\n",
       "      <td>0.018283</td>\n",
       "      <td>1</td>\n",
       "      <td>not shuffled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>143.103454</td>\n",
       "      <td>1.925904</td>\n",
       "      <td>0.016382</td>\n",
       "      <td>0.002895</td>\n",
       "      <td>0.000732</td>\n",
       "      <td>saga</td>\n",
       "      <td>elasticnet</td>\n",
       "      <td>800</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.100</td>\n",
       "      <td>{'tol': 0.000732, 'solver': 'saga', 'penalty':...</td>\n",
       "      <td>0.675150</td>\n",
       "      <td>0.676539</td>\n",
       "      <td>0.663889</td>\n",
       "      <td>0.653241</td>\n",
       "      <td>0.704630</td>\n",
       "      <td>0.674690</td>\n",
       "      <td>0.017186</td>\n",
       "      <td>2</td>\n",
       "      <td>not shuffled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.199692</td>\n",
       "      <td>0.292782</td>\n",
       "      <td>0.013051</td>\n",
       "      <td>0.001322</td>\n",
       "      <td>0.000565</td>\n",
       "      <td>saga</td>\n",
       "      <td>elasticnet</td>\n",
       "      <td>500</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.001</td>\n",
       "      <td>{'tol': 0.000565, 'solver': 'saga', 'penalty':...</td>\n",
       "      <td>0.109671</td>\n",
       "      <td>0.157797</td>\n",
       "      <td>0.108796</td>\n",
       "      <td>0.110648</td>\n",
       "      <td>0.107407</td>\n",
       "      <td>0.118864</td>\n",
       "      <td>0.019496</td>\n",
       "      <td>4</td>\n",
       "      <td>not shuffled</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  param_tol  \\\n",
       "0      56.461415      2.210524         0.015963        0.002591   0.000406   \n",
       "1       0.021851      0.002654         0.000000        0.000000   0.000105   \n",
       "2     195.807875      3.518417         0.015172        0.003087   0.000468   \n",
       "3     143.103454      1.925904         0.016382        0.002895   0.000732   \n",
       "4       7.199692      0.292782         0.013051        0.001322   0.000565   \n",
       "\n",
       "  param_solver param_penalty  param_max_iter  param_l1_ratio  param_C  \\\n",
       "0         saga    elasticnet             900             0.9    0.010   \n",
       "1    newton-cg    elasticnet             800             0.9    0.100   \n",
       "2         saga    elasticnet            1000             0.9    0.100   \n",
       "3         saga    elasticnet             800             0.9    0.100   \n",
       "4         saga    elasticnet             500             0.5    0.001   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "0  {'tol': 0.000406, 'solver': 'saga', 'penalty':...           0.378528   \n",
       "1  {'tol': 0.00010499999999999999, 'solver': 'new...                NaN   \n",
       "2  {'tol': 0.000468, 'solver': 'saga', 'penalty':...           0.673762   \n",
       "3  {'tol': 0.000732, 'solver': 'saga', 'penalty':...           0.675150   \n",
       "4  {'tol': 0.000565, 'solver': 'saga', 'penalty':...           0.109671   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
       "0           0.418325           0.395833           0.387963           0.443056   \n",
       "1                NaN                NaN                NaN                NaN   \n",
       "2           0.676539           0.663889           0.652778           0.707407   \n",
       "3           0.676539           0.663889           0.653241           0.704630   \n",
       "4           0.157797           0.108796           0.110648           0.107407   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score    model_type  \n",
       "0         0.404741        0.023240                3  not shuffled  \n",
       "1              NaN             NaN                5  not shuffled  \n",
       "2         0.674875        0.018283                1  not shuffled  \n",
       "3         0.674690        0.017186                2  not shuffled  \n",
       "4         0.118864        0.019496                4  not shuffled  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Opening saved cv scores after hyper-parameter tuning the model\n",
    "cv_scores = pd.read_csv(cv_outpath)\n",
    "shuffled_cv_scores = pd.read_csv(shuffled_cv_outpath)\n",
    "\n",
    "# add a column for both data frame indicating which model is shuffled\n",
    "cv_scores[\"model_type\"] = \"not shuffled\"\n",
    "shuffled_cv_scores[\"model_type\"] = \"shuffled\"\n",
    "\n",
    "# now concatenate both DataFrame into a single one and save it as csv file\n",
    "# this table will be used as the supplementary table\n",
    "concat_cv_score = pd.concat([cv_scores, shuffled_cv_scores])\n",
    "concat_cv_score.to_csv(modeling_dir / \"all_cv_scores.csv\", index=False)\n",
    "\n",
    "# display the concatenated cv score data\n",
    "concat_cv_score.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metadata-search",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
